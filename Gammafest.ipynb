{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7400aa0613f24eb1ae0861c005e4b72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7a348107b7b4915874239764f4fddc3",
              "IPY_MODEL_297295aafe9a46c5bc21c7b105f92d24",
              "IPY_MODEL_22c6b76c99f64fb0a1f7b1be7c58e6e4"
            ],
            "layout": "IPY_MODEL_fbd560fd85364d1780aaebb2b2ed6781"
          }
        },
        "d7a348107b7b4915874239764f4fddc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e241bbfa604909b47c49fd169f6414",
            "placeholder": "​",
            "style": "IPY_MODEL_cc3a55685c214645af52cf16c090944e",
            "value": "Map: 100%"
          }
        },
        "297295aafe9a46c5bc21c7b105f92d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f01ad9997074d87ae7c6716dd57604e",
            "max": 8584,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccb98451b0da40618c3f03629234019c",
            "value": 8584
          }
        },
        "22c6b76c99f64fb0a1f7b1be7c58e6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caad084561084c70a0e4549e08c1ca15",
            "placeholder": "​",
            "style": "IPY_MODEL_ef1abc43593945a2800ba2f45ebea8f3",
            "value": " 8584/8584 [00:05&lt;00:00, 1492.76 examples/s]"
          }
        },
        "fbd560fd85364d1780aaebb2b2ed6781": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e241bbfa604909b47c49fd169f6414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc3a55685c214645af52cf16c090944e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f01ad9997074d87ae7c6716dd57604e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb98451b0da40618c3f03629234019c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "caad084561084c70a0e4549e08c1ca15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1abc43593945a2800ba2f45ebea8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "l # Pastikan Anda sudah mengunggah file CSV ke Google Colab, kemudian jalankan kode berikut:\n",
        "\n",
        "# Import Library\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Mengunggah file yang diperlukan\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/balanced_train.csv')\n",
        "papers_metadata = pd.read_csv('/content/drive/MyDrive/papers_metadata.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Menggabungkan data train dan metadata untuk mendapatkan informasi lebih tentang paper\n",
        "train_data_merged = pd.merge(train_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "train_data_merged = pd.merge(train_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengatasi nilai NaN pada kolom 'title_paper' dan 'title_referenced_paper' sebelum fit_transform\n",
        "# Mengisi nilai NaN dengan string kosong\n",
        "train_data_merged['title_paper'] = train_data_merged['title_paper'].fillna('')\n",
        "train_data_merged['title_referenced_paper'] = train_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Menggunakan TF-IDF untuk mengubah judul menjadi fitur\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "# Menyusun fitur X dan label y untuk training\n",
        "X_train_titles = vectorizer.fit_transform(train_data_merged['title_paper'] + \" \" + train_data_merged['title_referenced_paper'])\n",
        "y_train = train_data_merged['is_referenced']\n",
        "\n",
        "# Melatih model Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_titles, y_train)\n",
        "\n",
        "# Menyiapkan data uji\n",
        "test_data_merged = pd.merge(test_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "test_data_merged = pd.merge(test_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Menghapus entri dengan nilai NaN pada kolom title_paper atau title_referenced_paper di data test\n",
        "test_data_merged = test_data_merged.dropna(subset=['title_paper', 'title_referenced_paper'])\n",
        "\n",
        "# Melakukan prediksi pada data uji\n",
        "X_test_titles = vectorizer.transform(test_data_merged['title_paper'] + \" \" + test_data_merged['title_referenced_paper'])\n",
        "y_pred = model.predict(X_test_titles)\n",
        "\n",
        "# Instead of directly assigning to test_data, create a new column in test_data_merged\n",
        "test_data_merged['is_referenced'] = y_pred\n",
        "\n",
        "# Now, merge this prediction back into the original test_data DataFrame\n",
        "test_data = pd.merge(test_data, test_data_merged[['id', 'is_referenced']], on='id', how='left')\n",
        "\n",
        "# Now you can safely proceed with the rest of your code\n",
        "# Menampilkan hasil prediksi\n",
        "print(test_data[['paper', 'referenced_paper', 'is_referenced']].head())\n",
        "# Evaluasi dengan MCC (jika data train memiliki label)\n",
        "mcc = matthews_corrcoef(y_train, model.predict(X_train_titles))\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "# Simpan hasil prediksi ke file CSV jika perlu\n",
        "test_data.to_csv('/content/predicted_test.csv', index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9MVBP3_I_dU",
        "outputId": "4a439692-3f19-4014-8c55-4860091b2415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   paper referenced_paper  is_referenced\n",
            "0  p0913            p3488            0.0\n",
            "1  p2971            p4337            1.0\n",
            "2  p2237            p1610            1.0\n",
            "3  p2876            p3212            1.0\n",
            "4  p2939            p1901            1.0\n",
            "Matthews Correlation Coefficient (MCC): 0.20487259032826732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan Anda sudah mengunggah file CSV ke Google Colab, kemudian jalankan kode berikut:\n",
        "\n",
        "# Import Library\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Mengunggah file yang diperlukan\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/balanced_train.csv')\n",
        "papers_metadata = pd.read_csv('/content/drive/MyDrive/papers_metadata.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Menggabungkan data train dan metadata untuk mendapatkan informasi lebih tentang paper\n",
        "train_data_merged = pd.merge(train_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "train_data_merged = pd.merge(train_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengatasi nilai NaN pada kolom 'title_paper' dan 'title_referenced_paper' sebelum fit_transform\n",
        "# Mengisi nilai NaN dengan string kosong\n",
        "train_data_merged['title_paper'] = train_data_merged['title_paper'].fillna('')\n",
        "train_data_merged['title_referenced_paper'] = train_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Menggunakan TF-IDF untuk mengubah judul menjadi fitur\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "# Menyusun fitur X dan label y untuk training\n",
        "X_train_titles = vectorizer.fit_transform(train_data_merged['title_paper'] + \" \" + train_data_merged['title_referenced_paper'])\n",
        "y_train = train_data_merged['is_referenced']\n",
        "\n",
        "# Melatih model Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_titles, y_train)\n",
        "\n",
        "# Menyiapkan data uji\n",
        "test_data_merged = pd.merge(test_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "test_data_merged = pd.merge(test_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Menghapus entri dengan nilai NaN pada kolom title_paper atau title_referenced_paper di data test\n",
        "test_data_merged = test_data_merged.dropna(subset=['title_paper', 'title_referenced_paper'])\n",
        "\n",
        "# Melakukan prediksi pada data uji\n",
        "X_test_titles = vectorizer.transform(test_data_merged['title_paper'] + \" \" + test_data_merged['title_referenced_paper'])\n",
        "y_pred = model.predict(X_test_titles)\n",
        "\n",
        "# Menambahkan hasil prediksi ke data uji\n",
        "test_data_merged['is_referenced'] = y_pred\n",
        "\n",
        "# Menggabungkan kembali hasil prediksi ke data asli\n",
        "test_data = pd.merge(test_data, test_data_merged[['id', 'is_referenced']], on='id', how='left')\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "print(test_data[['paper', 'referenced_paper', 'is_referenced']].head())\n",
        "\n",
        "# Evaluasi dengan MCC (jika data train memiliki label)\n",
        "mcc = matthews_corrcoef(y_train, model.predict(X_train_titles))\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "# Simpan hasil prediksi ke file CSV jika perlu\n",
        "test_data.to_csv('/content/predicted_test.csv', index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4s3tg16JCpM",
        "outputId": "2cb791d8-cd8c-4139-a2ac-85a0275e021e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   paper referenced_paper  is_referenced\n",
            "0  p0913            p3488            0.0\n",
            "1  p2971            p4337            1.0\n",
            "2  p2237            p1610            1.0\n",
            "3  p2876            p3212            0.0\n",
            "4  p2939            p1901            1.0\n",
            "Matthews Correlation Coefficient (MCC): 0.9949506203874302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Random Forest tanpa Row kosong\n",
        "\n",
        "# Import Library\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Mengunggah file yang diperlukan\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/balanced_train.csv')\n",
        "papers_metadata = pd.read_csv('/content/drive/MyDrive/papers_metadata.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Simpan salinan dari test_data original sebelum dimanipulasi\n",
        "original_test_data = test_data.copy()\n",
        "\n",
        "# Menggabungkan data train dan metadata untuk mendapatkan informasi lebih tentang paper\n",
        "train_data_merged = pd.merge(train_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "train_data_merged = pd.merge(train_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong\n",
        "train_data_merged['title_paper'] = train_data_merged['title_paper'].fillna('')\n",
        "train_data_merged['title_referenced_paper'] = train_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Menggunakan TF-IDF untuk mengubah judul menjadi fitur\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "# Menyusun fitur X dan label y untuk training\n",
        "X_train_titles = vectorizer.fit_transform(train_data_merged['title_paper'] + \" \" + train_data_merged['title_referenced_paper'])\n",
        "y_train = train_data_merged['is_referenced']\n",
        "\n",
        "# Melatih model Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_titles, y_train)\n",
        "\n",
        "# Menyiapkan data uji\n",
        "test_data_merged = pd.merge(test_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "test_data_merged = pd.merge(test_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Buat salinan ID dari dataset test yang lengkap\n",
        "test_ids = test_data_merged['id'].copy()\n",
        "\n",
        "# Mencatat indeks data dengan title yang tidak lengkap\n",
        "missing_indices = test_data_merged[test_data_merged['title_paper'].isna() | test_data_merged['title_referenced_paper'].isna()].index\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong, alih-alih menggunakan dropna\n",
        "test_data_merged['title_paper'] = test_data_merged['title_paper'].fillna('')\n",
        "test_data_merged['title_referenced_paper'] = test_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Melakukan prediksi pada semua data uji (tidak lagi menggunakan dropna)\n",
        "X_test_titles = vectorizer.transform(test_data_merged['title_paper'] + \" \" + test_data_merged['title_referenced_paper'])\n",
        "y_pred = model.predict(X_test_titles)\n",
        "\n",
        "# Menambahkan hasil prediksi ke data uji\n",
        "test_data_merged['is_referenced'] = y_pred\n",
        "\n",
        "# Update dataset asli dengan hasil prediksi\n",
        "result_data = pd.merge(original_test_data, test_data_merged[['id', 'is_referenced']], on='id', how='left')\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "print(result_data[['paper', 'referenced_paper', 'is_referenced']].head())\n",
        "print(f\"Total rows: {len(result_data)}\")\n",
        "print(f\"Rows with is_referenced: {result_data['is_referenced'].count()}\")\n",
        "print(f\"Missing values: {result_data['is_referenced'].isna().sum()}\")\n",
        "\n",
        "# Evaluasi dengan MCC (jika data train memiliki label)\n",
        "mcc = matthews_corrcoef(y_train, model.predict(X_train_titles))\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "# Simpan hasil prediksi ke file CSV\n",
        "result_data.to_csv('/content/predicted_test.csv', index=False)"
      ],
      "metadata": {
        "id": "A56dgidj96ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grusRSsp4zrx",
        "outputId": "9ee80154-f107-490c-acc5-7be1fd4f9e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2.post1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n",
            "Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2.post1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (291.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.7/291.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
            "Successfully installed nvidia-nccl-cu12-2.26.2.post1 xgboost-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Mengunggah file yang diperlukan\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/balanced_train.csv')\n",
        "papers_metadata = pd.read_csv('/content/drive/MyDrive/papers_metadata.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Menggabungkan data train dan metadata untuk mendapatkan informasi lebih tentang paper\n",
        "train_data_merged = pd.merge(train_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "train_data_merged = pd.merge(train_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengatasi nilai NaN pada kolom 'title_paper' dan 'title_referenced_paper' sebelum fit_transform\n",
        "# Mengisi nilai NaN dengan string kosong\n",
        "train_data_merged['title_paper'] = train_data_merged['title_paper'].fillna('')\n",
        "train_data_merged['title_referenced_paper'] = train_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Menggunakan TF-IDF untuk mengubah judul menjadi fitur\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "# Menyusun fitur X dan label y untuk training\n",
        "X_train_titles = vectorizer.fit_transform(train_data_merged['title_paper'] + \" \" + train_data_merged['title_referenced_paper'])\n",
        "y_train = train_data_merged['is_referenced']\n",
        "\n",
        "# Melatih model XGBoost\n",
        "model = XGBClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_titles, y_train)\n",
        "\n",
        "# Menyiapkan data uji\n",
        "test_data_merged = pd.merge(test_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "test_data_merged = pd.merge(test_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Menghapus entri dengan nilai NaN pada kolom title_paper atau title_referenced_paper di data test\n",
        "test_data_merged = test_data_merged.dropna(subset=['title_paper', 'title_referenced_paper'])\n",
        "\n",
        "# Melakukan prediksi pada data uji\n",
        "X_test_titles = vectorizer.transform(test_data_merged['title_paper'] + \" \" + test_data_merged['title_referenced_paper'])\n",
        "y_pred = model.predict(X_test_titles)\n",
        "\n",
        "# Menambahkan hasil prediksi ke data uji\n",
        "test_data_merged['is_referenced'] = y_pred\n",
        "\n",
        "# Menggabungkan kembali hasil prediksi ke data asli\n",
        "test_data = pd.merge(test_data, test_data_merged[['id', 'is_referenced']], on='id', how='left')\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "print(test_data[['paper', 'referenced_paper', 'is_referenced']].head())\n",
        "\n",
        "# Evaluasi dengan MCC (jika data train memiliki label)\n",
        "mcc = matthews_corrcoef(y_train, model.predict(X_train_titles))\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "# Simpan hasil prediksi ke file CSV jika perlu\n",
        "test_data.to_csv('/content/predicted_test.csv', index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL3OnMSW4xMu",
        "outputId": "5d1fb46a-cc60-4357-c617-3c467ac9cab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   paper referenced_paper  is_referenced\n",
            "0  p0913            p3488            0.0\n",
            "1  p2971            p4337            0.0\n",
            "2  p2237            p1610            0.0\n",
            "3  p2876            p3212            1.0\n",
            "4  p2939            p1901            0.0\n",
            "Matthews Correlation Coefficient (MCC): 0.3019123137588781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Model GNN\n",
        "!pip install torch-geometric\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
        "from torch_geometric.data import Data\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Mengunggah file yang diperlukan\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/balanced_train1.csv')\n",
        "papers_metadata = pd.read_csv('/content/drive/MyDrive/papers_metadata.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Menggabungkan data train dan metadata untuk mendapatkan informasi lebih tentang paper\n",
        "train_data_merged = pd.merge(train_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "train_data_merged = pd.merge(train_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong\n",
        "train_data_merged['title_paper'] = train_data_merged['title_paper'].fillna('')\n",
        "train_data_merged['title_referenced_paper'] = train_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Menggunakan TF-IDF untuk mengubah judul menjadi fitur\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_titles = vectorizer.fit_transform(train_data_merged['title_paper'] + \" \" + train_data_merged['title_referenced_paper'])\n",
        "y_train = train_data_merged['is_referenced']\n",
        "\n",
        "# GNN expects a graph structure, so let's create nodes and edges based on paper relationships\n",
        "\n",
        "# Define an edge list based on the relationship between papers (i.e., referencing each other)\n",
        "paper_id_mapping = {paper_id: i for i, paper_id in enumerate(papers_metadata['paper_id'].unique())}\n",
        "\n",
        "edges = []\n",
        "for i, row in train_data_merged.iterrows():\n",
        "    paper_id = paper_id_mapping.get(row['paper'])\n",
        "    referenced_paper_id = paper_id_mapping.get(row['referenced_paper'])\n",
        "    if paper_id is not None and referenced_paper_id is not None:\n",
        "        edges.append([paper_id, referenced_paper_id])\n",
        "\n",
        "# Convert the edge list to a torch tensor\n",
        "edges_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Create a Data object for PyTorch Geometric\n",
        "num_nodes = len(papers_metadata)  # Number of unique papers\n",
        "features = torch.tensor(X_train_titles.toarray(), dtype=torch.float)  # TF-IDF features as node features\n",
        "labels = torch.tensor(y_train.values, dtype=torch.long)  # Labels for each paper\n",
        "\n",
        "# Create the data object for PyTorch Geometric\n",
        "data = Data(x=features, edge_index=edges_tensor, y=labels)\n",
        "\n",
        "# Define a simple GCN model for node classification\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_dim = features.shape[1]  # TF-IDF feature dimension\n",
        "hidden_dim = 32  # Hidden dimension\n",
        "output_dim = 2   # Output dimension for binary classification\n",
        "\n",
        "model = GCN(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(100):  # Train for 100 epochs\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = criterion(output, data.y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the model on training data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(data)\n",
        "    pred = output.argmax(dim=1)  # Get the class with the highest score\n",
        "    mcc = matthews_corrcoef(data.y.numpy(), pred.numpy())\n",
        "    print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n",
        "\n",
        "# Now for the test data predictions\n",
        "# Prepare the test data similar to how we prepared the train data\n",
        "test_data_merged = pd.merge(test_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "test_data_merged = pd.merge(test_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong di test data\n",
        "test_data_merged['title_paper'] = test_data_merged['title_paper'].fillna('')\n",
        "test_data_merged['title_referenced_paper'] = test_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Transform the test titles using the trained vectorizer\n",
        "X_test_titles = vectorizer.transform(test_data_merged['title_paper'] + \" \" + test_data_merged['title_referenced_paper'])\n",
        "\n",
        "# Create the test Data object (similar to the train Data object)\n",
        "test_features = torch.tensor(X_test_titles.toarray(), dtype=torch.float)\n",
        "test_data_obj = Data(x=test_features, edge_index=edges_tensor, y=None)\n",
        "\n",
        "# Get predictions for test data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_output = model(test_data_obj)\n",
        "    test_pred = test_output.argmax(dim=1)\n",
        "\n",
        "# Save predictions to the test data\n",
        "test_data_merged['is_referenced'] = test_pred.numpy()\n",
        "\n",
        "# Save the test data with predictions to CSV\n",
        "test_data_merged[['paper', 'referenced_paper', 'is_referenced']].to_csv('/content/predicted_test.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to /content/predicted_test.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tXmdHYz0-RDr",
        "outputId": "23cd0a62-3091-4079-fe44-eb41d334346e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-cluster\n",
            "  Using cached torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-spline-conv\n",
            "  Using cached torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster, torch-spline-conv\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=545106 sha256=8b0d538d96267a118427009de730d902a1158921386c2262acca4d607ffe482b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch_geometric' has no attribute 'typing' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1d5579b02eda>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch-geometric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch-scatter torch-sparse torch-cluster torch-spline-conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_home_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_home_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_mps_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_xpu_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0misinstance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_torch_instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/isinstance.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWITH_PT20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch_geometric' has no attribute 'typing' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages if not already installed\n",
        "!pip install xgboost torch-geometric scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score, f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.optim as optim\n",
        "\n",
        "# Mengunggah file yang diperlukan\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/balanced_train1.csv')\n",
        "papers_metadata = pd.read_csv('/content/drive/MyDrive/papers_metadata.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Menggabungkan data train dan metadata untuk mendapatkan informasi lebih tentang paper\n",
        "train_data_merged = pd.merge(train_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "train_data_merged = pd.merge(train_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong\n",
        "train_data_merged['title_paper'] = train_data_merged['title_paper'].fillna('')\n",
        "train_data_merged['title_referenced_paper'] = train_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Menggunakan TF-IDF untuk mengubah judul menjadi fitur\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_titles = vectorizer.fit_transform(train_data_merged['title_paper'] + \" \" + train_data_merged['title_referenced_paper'])\n",
        "y_train = train_data_merged['is_referenced']\n",
        "\n",
        "# Prepare Test Data (same process as for training data)\n",
        "test_data_merged = pd.merge(test_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "test_data_merged = pd.merge(test_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong di test data\n",
        "test_data_merged['title_paper'] = test_data_merged['title_paper'].fillna('')\n",
        "test_data_merged['title_referenced_paper'] = test_data_merged['title_referenced_paper'].fillna('')\n",
        "X_test_titles = vectorizer.transform(test_data_merged['title_paper'] + \" \" + test_data_merged['title_referenced_paper'])\n",
        "\n",
        "### 1. Random Forest Model\n",
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_titles, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred_rf = rf_model.predict(X_test_titles)\n",
        "y_prob_rf = rf_model.predict_proba(X_test_titles)[:, 1]  # For AUC calculation\n",
        "\n",
        "# Evaluate with MCC, AUC, F1 Score, and Accuracy\n",
        "mcc_rf = matthews_corrcoef(y_train, rf_model.predict(X_train_titles))\n",
        "auc_rf = roc_auc_score(y_train, rf_model.predict_proba(X_train_titles)[:, 1])\n",
        "f1_rf = f1_score(y_train, rf_model.predict(X_train_titles))\n",
        "accuracy_rf = accuracy_score(y_train, rf_model.predict(X_train_titles))\n",
        "\n",
        "print(f\"Random Forest Model - MCC: {mcc_rf}\")\n",
        "print(f\"Random Forest Model - AUC: {auc_rf}\")\n",
        "print(f\"Random Forest Model - F1 Score: {f1_rf}\")\n",
        "print(f\"Random Forest Model - Accuracy: {accuracy_rf}\")\n",
        "\n",
        "# Save Random Forest predictions\n",
        "test_data_merged['is_referenced_rf'] = y_pred_rf\n",
        "test_data_merged[['paper', 'referenced_paper', 'is_referenced_rf']].to_csv('/content/predicted_rf_test.csv', index=False)\n",
        "\n",
        "### 2. XGBoost Model\n",
        "# XGBoost Model\n",
        "xgb_model = XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model.fit(X_train_titles, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred_xgb = xgb_model.predict(X_test_titles)\n",
        "y_prob_xgb = xgb_model.predict_proba(X_test_titles)[:, 1]  # For AUC calculation\n",
        "\n",
        "# Evaluate with MCC, AUC, F1 Score, and Accuracy\n",
        "mcc_xgb = matthews_corrcoef(y_train, xgb_model.predict(X_train_titles))\n",
        "auc_xgb = roc_auc_score(y_train, xgb_model.predict_proba(X_train_titles)[:, 1])\n",
        "f1_xgb = f1_score(y_train, xgb_model.predict(X_train_titles))\n",
        "accuracy_xgb = accuracy_score(y_train, xgb_model.predict(X_train_titles))\n",
        "\n",
        "print(f\"XGBoost Model - MCC: {mcc_xgb}\")\n",
        "print(f\"XGBoost Model - AUC: {auc_xgb}\")\n",
        "print(f\"XGBoost Model - F1 Score: {f1_xgb}\")\n",
        "print(f\"XGBoost Model - Accuracy: {accuracy_xgb}\")\n",
        "\n",
        "# Save XGBoost predictions\n",
        "test_data_merged['is_referenced_xgb'] = y_pred_xgb\n",
        "test_data_merged[['paper', 'referenced_paper', 'is_referenced_xgb']].to_csv('/content/predicted_xgb_test.csv', index=False)\n",
        "\n",
        "### 3. Graph Neural Network (GNN) Model\n",
        "# Convert 'paper' and 'referenced_paper' to numeric paper IDs for graph construction\n",
        "paper_id_mapping = {paper_id: i for i, paper_id in enumerate(papers_metadata['paper_id'].unique())}\n",
        "\n",
        "edges = []\n",
        "for i, row in train_data_merged.iterrows():\n",
        "    paper_id = paper_id_mapping.get(row['paper'])\n",
        "    referenced_paper_id = paper_id_mapping.get(row['referenced_paper'])\n",
        "    if paper_id is not None and referenced_paper_id is not None:\n",
        "        edges.append([paper_id, referenced_paper_id])\n",
        "\n",
        "# Convert the edge list to a torch tensor\n",
        "edges_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Now create a Data object for PyTorch Geometric\n",
        "num_nodes = len(papers_metadata)  # Number of unique papers\n",
        "features = torch.tensor(X_train_titles.toarray(), dtype=torch.float)  # TF-IDF features as node features\n",
        "labels = torch.tensor(y_train.values, dtype=torch.long)  # Labels for each paper\n",
        "\n",
        "# Create the data object for PyTorch Geometric\n",
        "data = Data(x=features, edge_index=edges_tensor, y=labels)\n",
        "\n",
        "# Define a simple GCN model for node classification\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_dim = features.shape[1]  # TF-IDF feature dimension\n",
        "hidden_dim = 32  # Hidden dimension\n",
        "output_dim = 2   # Output dimension for binary classification\n",
        "\n",
        "gnn_model = GCN(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the GNN model\n",
        "gnn_model.train()\n",
        "for epoch in range(100):  # Train for 100 epochs\n",
        "    optimizer.zero_grad()\n",
        "    output = gnn_model(data)\n",
        "    loss = criterion(output, data.y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"GNN Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the GNN model on training data\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    output = gnn_model(data)\n",
        "    pred_gnn = output.argmax(dim=1)  # Get the class with the highest score\n",
        "    mcc_gnn = matthews_corrcoef(data.y.numpy(), pred_gnn.numpy())\n",
        "    auc_gnn = roc_auc_score(data.y.numpy(), output[:, 1].numpy())\n",
        "    f1_gnn = f1_score(data.y.numpy(), pred_gnn.numpy())\n",
        "    accuracy_gnn = accuracy_score(data.y.numpy(), pred_gnn.numpy())\n",
        "    print(f\"GNN Model - MCC: {mcc_gnn}\")\n",
        "    print(f\"GNN Model - AUC: {auc_gnn}\")\n",
        "    print(f\"GNN Model - F1 Score: {f1_gnn}\")\n",
        "    print(f\"GNN Model - Accuracy: {accuracy_gnn}\")\n",
        "\n",
        "# Now for the test data predictions with GNN\n",
        "# Create the test Data object for GNN (similar to the train Data object)\n",
        "test_features = torch.tensor(X_test_titles.toarray(), dtype=torch.float)\n",
        "test_data_obj = Data(x=test_features, edge_index=edges_tensor, y=None)\n",
        "\n",
        "# Get predictions for test data\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_output = gnn_model(test_data_obj)\n",
        "    test_pred = test_output.argmax(dim=1)\n",
        "\n",
        "# Save GNN predictions\n",
        "test_data_merged['is_referenced_gnn'] = test_pred.numpy()\n",
        "test_data_merged[['paper', 'referenced_paper', 'is_referenced_gnn']].to_csv('/content/predicted_gnn_test.csv', index=False)\n",
        "\n",
        "print(\"GNN predictions saved to /content/predicted_gnn_test.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nKFj6IASp4b",
        "outputId": "2a7adb4f-d561-4885-e2bc-dea5db6c4a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.26.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Random Forest Model - MCC: 0.9990680335507922\n",
            "Random Forest Model - AUC: 0.9999993485789032\n",
            "Random Forest Model - F1 Score: 0.9995340167753961\n",
            "Random Forest Model - Accuracy: 0.9995340167753961\n",
            "XGBoost Model - MCC: 0.7246952094864898\n",
            "XGBoost Model - AUC: 0.9423078948328409\n",
            "XGBoost Model - F1 Score: 0.8548131921625898\n",
            "XGBoost Model - Accuracy: 0.8610205032618826\n",
            "GNN Epoch 1, Loss: 0.6933016180992126\n",
            "GNN Epoch 11, Loss: 0.5463607907295227\n",
            "GNN Epoch 21, Loss: 0.46431979537010193\n",
            "GNN Epoch 31, Loss: 0.40177521109580994\n",
            "GNN Epoch 41, Loss: 0.3522067666053772\n",
            "GNN Epoch 51, Loss: 0.31090638041496277\n",
            "GNN Epoch 61, Loss: 0.27589499950408936\n",
            "GNN Epoch 71, Loss: 0.2467886358499527\n",
            "GNN Epoch 81, Loss: 0.2236015647649765\n",
            "GNN Epoch 91, Loss: 0.2033093422651291\n",
            "GNN Model - MCC: 0.8667334087443725\n",
            "GNN Model - AUC: 0.9794234261014879\n",
            "GNN Model - F1 Score: 0.9332555425904318\n",
            "GNN Model - Accuracy: 0.9333643988816402\n",
            "GNN predictions saved to /content/predicted_gnn_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages if not already installed\n",
        "!pip install xgboost torch-geometric scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score, f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.optim as optim\n",
        "\n",
        "# Mengunggah file yang diperlukan\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/sampled_train.csv')\n",
        "papers_metadata = pd.read_csv('/content/drive/MyDrive/papers_metadata.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Menggabungkan data train dan metadata untuk mendapatkan informasi lebih tentang paper\n",
        "train_data_merged = pd.merge(train_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "train_data_merged = pd.merge(train_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong\n",
        "train_data_merged['title_paper'] = train_data_merged['title_paper'].fillna('')\n",
        "train_data_merged['title_referenced_paper'] = train_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Menggunakan TF-IDF untuk mengubah judul menjadi fitur\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_titles = vectorizer.fit_transform(train_data_merged['title_paper'] + \" \" + train_data_merged['title_referenced_paper'])\n",
        "y_train = train_data_merged['is_referenced']\n",
        "\n",
        "# Prepare Test Data (same process as for training data)\n",
        "test_data_merged = pd.merge(test_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "test_data_merged = pd.merge(test_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong di test data\n",
        "test_data_merged['title_paper'] = test_data_merged['title_paper'].fillna('')\n",
        "test_data_merged['title_referenced_paper'] = test_data_merged['title_referenced_paper'].fillna('')\n",
        "X_test_titles = vectorizer.transform(test_data_merged['title_paper'] + \" \" + test_data_merged['title_referenced_paper'])\n",
        "\n",
        "### 1. Random Forest Model\n",
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_titles, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred_rf = rf_model.predict(X_test_titles)\n",
        "y_prob_rf = rf_model.predict_proba(X_test_titles)[:, 1]  # For AUC calculation\n",
        "\n",
        "# Evaluate with MCC, AUC, F1 Score, and Accuracy\n",
        "mcc_rf = matthews_corrcoef(y_train, rf_model.predict(X_train_titles))\n",
        "auc_rf = roc_auc_score(y_train, rf_model.predict_proba(X_train_titles)[:, 1])\n",
        "f1_rf = f1_score(y_train, rf_model.predict(X_train_titles))\n",
        "accuracy_rf = accuracy_score(y_train, rf_model.predict(X_train_titles))\n",
        "\n",
        "print(f\"Random Forest Model - MCC: {mcc_rf}\")\n",
        "print(f\"Random Forest Model - AUC: {auc_rf}\")\n",
        "print(f\"Random Forest Model - F1 Score: {f1_rf}\")\n",
        "print(f\"Random Forest Model - Accuracy: {accuracy_rf}\")\n",
        "\n",
        "# Save Random Forest predictions\n",
        "test_data_merged['is_referenced_rf'] = y_pred_rf\n",
        "test_data_merged[['paper', 'referenced_paper', 'is_referenced_rf']].to_csv('/content/predicted_rf_test.csv', index=False)\n",
        "\n",
        "### 2. XGBoost Model\n",
        "# XGBoost Model\n",
        "xgb_model = XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model.fit(X_train_titles, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred_xgb = xgb_model.predict(X_test_titles)\n",
        "y_prob_xgb = xgb_model.predict_proba(X_test_titles)[:, 1]  # For AUC calculation\n",
        "\n",
        "# Evaluate with MCC, AUC, F1 Score, and Accuracy\n",
        "mcc_xgb = matthews_corrcoef(y_train, xgb_model.predict(X_train_titles))\n",
        "auc_xgb = roc_auc_score(y_train, xgb_model.predict_proba(X_train_titles)[:, 1])\n",
        "f1_xgb = f1_score(y_train, xgb_model.predict(X_train_titles))\n",
        "accuracy_xgb = accuracy_score(y_train, xgb_model.predict(X_train_titles))\n",
        "\n",
        "print(f\"XGBoost Model - MCC: {mcc_xgb}\")\n",
        "print(f\"XGBoost Model - AUC: {auc_xgb}\")\n",
        "print(f\"XGBoost Model - F1 Score: {f1_xgb}\")\n",
        "print(f\"XGBoost Model - Accuracy: {accuracy_xgb}\")\n",
        "\n",
        "# Save XGBoost predictions\n",
        "test_data_merged['is_referenced_xgb'] = y_pred_xgb\n",
        "test_data_merged[['paper', 'referenced_paper', 'is_referenced_xgb']].to_csv('/content/predicted_xgb_test.csv', index=False)\n",
        "\n",
        "### 3. Graph Neural Network (GNN) Model\n",
        "# Convert 'paper' and 'referenced_paper' to numeric paper IDs for graph construction\n",
        "paper_id_mapping = {paper_id: i for i, paper_id in enumerate(papers_metadata['paper_id'].unique())}\n",
        "\n",
        "edges = []\n",
        "for i, row in train_data_merged.iterrows():\n",
        "    paper_id = paper_id_mapping.get(row['paper'])\n",
        "    referenced_paper_id = paper_id_mapping.get(row['referenced_paper'])\n",
        "    if paper_id is not None and referenced_paper_id is not None:\n",
        "        edges.append([paper_id, referenced_paper_id])\n",
        "\n",
        "# Convert the edge list to a torch tensor\n",
        "edges_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Now create a Data object for PyTorch Geometric\n",
        "num_nodes = len(papers_metadata)  # Number of unique papers\n",
        "features = torch.tensor(X_train_titles.toarray(), dtype=torch.float)  # TF-IDF features as node features\n",
        "labels = torch.tensor(y_train.values, dtype=torch.long)  # Labels for each paper\n",
        "\n",
        "# Create the data object for PyTorch Geometric\n",
        "data = Data(x=features, edge_index=edges_tensor, y=labels)\n",
        "\n",
        "# Define a simple GCN model for node classification\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_dim = features.shape[1]  # TF-IDF feature dimension\n",
        "hidden_dim = 32  # Hidden dimension\n",
        "output_dim = 2   # Output dimension for binary classification\n",
        "\n",
        "gnn_model = GCN(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the GNN model\n",
        "gnn_model.train()\n",
        "for epoch in range(100):  # Train for 100 epochs\n",
        "    optimizer.zero_grad()\n",
        "    output = gnn_model(data)\n",
        "    loss = criterion(output, data.y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"GNN Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the GNN model on training data\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    output = gnn_model(data)\n",
        "    pred_gnn = output.argmax(dim=1)  # Get the class with the highest score\n",
        "    mcc_gnn = matthews_corrcoef(data.y.numpy(), pred_gnn.numpy())\n",
        "    auc_gnn = roc_auc_score(data.y.numpy(), output[:, 1].numpy())\n",
        "    f1_gnn = f1_score(data.y.numpy(), pred_gnn.numpy())\n",
        "    accuracy_gnn = accuracy_score(data.y.numpy(), pred_gnn.numpy())\n",
        "    print(f\"GNN Model - MCC: {mcc_gnn}\")\n",
        "    print(f\"GNN Model - AUC: {auc_gnn}\")\n",
        "    print(f\"GNN Model - F1 Score: {f1_gnn}\")\n",
        "    print(f\"GNN Model - Accuracy: {accuracy_gnn}\")\n",
        "\n",
        "# Now for the test data predictions with GNN\n",
        "# Create the test Data object for GNN (similar to the train Data object)\n",
        "test_features = torch.tensor(X_test_titles.toarray(), dtype=torch.float)\n",
        "test_data_obj = Data(x=test_features, edge_index=edges_tensor, y=None)\n",
        "\n",
        "# Get predictions for test data\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_output = gnn_model(test_data_obj)\n",
        "    test_pred = test_output.argmax(dim=1)\n",
        "\n",
        "# Save GNN predictions\n",
        "test_data_merged['is_referenced_gnn'] = test_pred.numpy()\n",
        "test_data_merged[['paper', 'referenced_paper', 'is_referenced_gnn']].to_csv('/content/predicted_gnn_test.csv', index=False)\n",
        "\n",
        "print(\"GNN predictions saved to /content/predicted_gnn_test.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meARmC0ZZQaP",
        "outputId": "29b89d08-bfae-4322-d2ca-16a84bc0c64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Random Forest Model - MCC: 0.9768849705341753\n",
            "Random Forest Model - AUC: 0.9999525543287588\n",
            "Random Forest Model - F1 Score: 0.9769650914272144\n",
            "Random Forest Model - Accuracy: 0.9994757479374464\n",
            "XGBoost Model - MCC: 0.227826053183956\n",
            "XGBoost Model - AUC: 0.9031798758743983\n",
            "XGBoost Model - F1 Score: 0.10982279588711441\n",
            "XGBoost Model - Accuracy: 0.9890042183374724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yI92xF5Y9-rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch scikit-learn imbalanced-learn\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH2lPQZhiatI",
        "outputId": "3e62d27f-82cd-42b3-c60a-bcf145b4ad1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Mengunggah file yang diperlukan\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/balanced_train1.csv')\n",
        "papers_metadata = pd.read_csv('/content/drive/MyDrive/papers_metadata.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Menggabungkan data train dan metadata untuk mendapatkan informasi lebih tentang paper\n",
        "train_data_merged = pd.merge(train_data, papers_metadata[['paper_id', 'title']], left_on='paper', right_on='paper_id', how='left')\n",
        "train_data_merged = pd.merge(train_data_merged, papers_metadata[['paper_id', 'title']], left_on='referenced_paper', right_on='paper_id', suffixes=('_paper', '_referenced_paper'), how='left')\n",
        "\n",
        "# Mengisi nilai NaN dengan string kosong\n",
        "train_data_merged['title_paper'] = train_data_merged['title_paper'].fillna('')\n",
        "train_data_merged['title_referenced_paper'] = train_data_merged['title_referenced_paper'].fillna('')\n",
        "\n",
        "# Gabungkan judul paper dan referensi untuk input BERT\n",
        "train_data_merged['input_text'] = train_data_merged['title_paper'] + \" [SEP] \" + train_data_merged['title_referenced_paper']\n",
        "\n",
        "# Tokenization for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the input text, ensuring padding and truncation\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "# Convert data into Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_data_merged[['input_text', 'is_referenced']])\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Load BERT Model for Sequence Classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/results',          # output directory\n",
        "    num_train_epochs=3,                # Total number of training epochs\n",
        "    per_device_train_batch_size=8,     # Batch size per device during training\n",
        "    per_device_eval_batch_size=8,      # Batch size per device during evaluation\n",
        "    learning_rate=2e-5,                 # Learning rate\n",
        "    weight_decay=0.01,                  # Strength of weight decay\n",
        "    logging_dir='/content/logs',          # Directory for storing logs\n",
        "    # The 'evaluation_strategy' argument is replaced with 'eval_steps'\n",
        "    # to specify the number of steps between evaluations.\n",
        "    # Here, it's set to evaluate every 500 steps.\n",
        "    # Adjust this value according to your needs.\n",
        "    eval_steps=500\n",
        ")\n",
        "\n",
        "# Define a compute_metrics function to evaluate the model\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = torch.argmax(torch.tensor(predictions), dim=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    auc = roc_auc_score(labels, predictions)\n",
        "    mcc = matthews_corrcoef(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'mcc': mcc\n",
        "    }\n",
        "\n",
        "# Define a custom Trainer class with a modified training_step\n",
        "class CustomTrainer(Trainer):\n",
        "    def training_step(self, model, inputs):\n",
        "        \"\"\"\n",
        "        Perform a training step on a batch of inputs.\n",
        "\n",
        "        Subclass and override to inject custom behavior.\n",
        "        \"\"\"\n",
        "        model.train()\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # Calculate loss using logits and labels (this is the key change)\n",
        "        with self.compute_loss_context_manager():\n",
        "            loss = model(**inputs).loss\n",
        "\n",
        "        # Rest of the training step (update gradients, etc.)\n",
        "        if self.args.gradient_accumulation_steps > 1:\n",
        "            loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "        if self.use_amp:\n",
        "            self.scaler.scale(loss).backward()\n",
        "        elif self.use_apex:\n",
        "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        elif self.deepspeed:\n",
        "            loss = self.deepspeed.backward(loss)\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        return loss.detach()\n",
        "\n",
        "# Create a CustomTrainer instance\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=train_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluating on the test set\n",
        "test_data_merged['input_text'] = test_data_merged['title_paper'] + \" [SEP] \" + test_data_merged['title_referenced_paper']\n",
        "test_dataset = Dataset.from_pandas(test_data_merged[['input_text', 'is_referenced']])\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Get predictions and labels\n",
        "predicted_labels = torch.argmax(torch.tensor(predictions.predictions), dim=1)\n",
        "true_labels = torch.tensor(predictions.label_ids)\n",
        "\n",
        "# Evaluate predictions\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "auc = roc_auc_score(true_labels, predicted_labels)\n",
        "mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"AUC: {auc}\")\n",
        "print(f\"MCC: {mcc}\")\n",
        "\n",
        "# Save predictions to CSV\n",
        "test_data_merged['is_referenced'] = predicted_labels.numpy()\n",
        "test_data_merged[['id', 'is_referenced']].to_csv('/content/predicted_test_bert.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to /content/predicted_test_bert.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "7400aa0613f24eb1ae0861c005e4b72e",
            "d7a348107b7b4915874239764f4fddc3",
            "297295aafe9a46c5bc21c7b105f92d24",
            "22c6b76c99f64fb0a1f7b1be7c58e6e4",
            "fbd560fd85364d1780aaebb2b2ed6781",
            "31e241bbfa604909b47c49fd169f6414",
            "cc3a55685c214645af52cf16c090944e",
            "2f01ad9997074d87ae7c6716dd57604e",
            "ccb98451b0da40618c3f03629234019c",
            "caad084561084c70a0e4549e08c1ca15",
            "ef1abc43593945a2800ba2f45ebea8f3"
          ]
        },
        "id": "1e6ESZpe4NHL",
        "outputId": "dc147d57-e428-44d1-ebad-9a49629098c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8584 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7400aa0613f24eb1ae0861c005e4b72e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    }
  ]
}